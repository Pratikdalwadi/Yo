import OpenAI from "openai";
import { GoogleGenerativeAI } from "@google/generative-ai";
import fs from "fs";
import { ExtractionResult, extractionResultSchema } from "@shared/schema";
// @ts-ignore - pdf-parse-debugging-disabled doesn't have types
import pdfParse from 'pdf-parse-debugging-disabled';
import * as pdfjsLib from 'pdfjs-dist/legacy/build/pdf.js';
// @ts-ignore - tesseract.js import
import Tesseract from 'tesseract.js';
import { vlmSemanticEngine } from './vlmSemanticEngine';

// the newest OpenAI model is "gpt-5" which was released August 7, 2025. do not change this unless explicitly requested by the user
// Note: We now create providers dynamically with API keys

export interface AIProvider {
  extractData(filePath: string, options: ExtractionOptions, apiKey?: string): Promise<ExtractionResult>;
}

export interface ExtractionOptions {
  extractionMode: 'smart_table' | 'full_text' | 'form_fields' | 'custom' | 'smart_image' | 'automatic_schema' | 'comprehensive' | 'vlm_layout_aware';
  preserveFormatting: boolean;
  includeConfidence: boolean;
  customSchema?: any;
  openaiApiKey?: string;
  geminiApiKey?: string;
}

class OpenAIProvider implements AIProvider {
  async extractData(filePath: string, options: ExtractionOptions, apiKey?: string): Promise<ExtractionResult> {
    try {
      const fileBuffer = fs.readFileSync(filePath);
      const base64Image = fileBuffer.toString('base64');
      const mimeType = this.getMimeType(filePath);

      // Use provided API key or fallback to environment variables
      const effectiveApiKey = apiKey || options.openaiApiKey || process.env.OPENAI_API_KEY || process.env.OPENAI_KEY || "";
      
      if (!effectiveApiKey) {
        throw new Error("OpenAI API key is required. Please provide an API key or set the OPENAI_API_KEY environment variable.");
      }
      
      const openaiInstance = new OpenAI({ apiKey: effectiveApiKey });

      // ADVANCED MULTI-PASS EXTRACTION PIPELINE
      return await this.performMultiPassExtraction(openaiInstance, filePath, fileBuffer, mimeType, options);
    } catch (error) {
      throw new Error(`OpenAI extraction failed: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  // ADVANCED MULTI-PASS EXTRACTION METHOD FOR ULTRA-PRECISION
  private async performMultiPassExtraction(
    openaiInstance: OpenAI, 
    filePath: string, 
    fileBuffer: Buffer, 
    mimeType: string, 
    options: ExtractionOptions
  ): Promise<ExtractionResult> {
    try {
      console.log(`üöÄ Starting ultra-precision extraction for: ${filePath}`);
      
      // PHASE 1: ROBUST TEXT EXTRACTION WITH COORDINATES
      const textData = await this.performAdvancedTextExtraction(fileBuffer, mimeType, filePath);
      console.log(`üìä Text extraction: ${textData.totalTextItems} items using ${textData.extractionMethod}`);
      
      // CHECK FOR VLM LAYOUT-AWARE MODE
      if (options.extractionMode === 'vlm_layout_aware') {
        return await this.performVLMLayoutAwareExtraction(openaiInstance, textData, options);
      }
      
      // PHASE 2: OPTIMIZED LAYOUT ANALYSIS
      const layoutAnalysis = await this.performOptimizedLayoutAnalysis(openaiInstance, textData, options);
      console.log(`üîç Layout analysis identified ${layoutAnalysis.document_type} with ${layoutAnalysis.layout_structure?.text_regions?.length || 0} regions`);
      
      // PHASE 3: CONTENT EXTRACTION WITH POSITIONING (USING EXISTING METHOD)
      const contentExtraction = await this.performPositionalExtraction(openaiInstance, textData, layoutAnalysis, options);
      console.log(`üìù Content extraction completed with ${contentExtraction.confidence || 0}% confidence`);
      
      // PHASE 4: QUALITY VALIDATION AND ENHANCEMENT (USING EXISTING METHOD)
      const validatedResult = await this.performAdvancedQualityValidation(openaiInstance, contentExtraction, textData, options);
      console.log(`‚úÖ Quality validation completed - Final confidence: ${(validatedResult.metadata as any)?.coverage_metrics?.overall_coverage || 0}%`);
      
      return validatedResult;
    } catch (error) {
      console.warn('üîÑ Multi-pass extraction failed, falling back to enhanced single-pass:', error);
      return await this.performEnhancedSinglePass(openaiInstance, fileBuffer, mimeType, options);
    }
  }

  // PHASE 1: ROBUST PDF TEXT EXTRACTION WITH ACCURATE COORDINATES
  private async performAdvancedTextExtraction(fileBuffer: Buffer, mimeType: string, filePath: string): Promise<any> {
    try {
      if (mimeType === 'application/pdf') {
        return await this.extractPDFTextWithCoordinates(fileBuffer);
      } else {
        // For images, use OCR with robust worker management
        return await this.extractImageTextWithOCR(fileBuffer);
      }
    } catch (error) {
      console.error('Text extraction failed:', error);
      throw new Error(`Text extraction failed: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  // PDF text extraction using pdfjs-dist (pure-JS, no OS dependencies)
  private async extractPDFTextWithCoordinates(fileBuffer: Buffer): Promise<any> {
    try {
      const loadingTask = pdfjsLib.getDocument({ data: fileBuffer });
      const pdf = await loadingTask.promise;
      
      const pages = [];
      let totalTextItems = 0;
      
      for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {
        const page = await pdf.getPage(pageNum);
        const viewport = page.getViewport({ scale: 1.0 });
        const textContent = await page.getTextContent();
        
        // Extract text items with accurate coordinates
        const textItems = textContent.items.map((item: any) => {
          // Use actual page dimensions for accurate normalization
          const x0 = item.transform[4] / viewport.width;
          const y0 = (viewport.height - item.transform[5] - item.height) / viewport.height; // PDF Y is bottom-up
          const x1 = (item.transform[4] + item.width) / viewport.width;
          const y1 = (viewport.height - item.transform[5]) / viewport.height;
          
          return {
            text: item.str || '',
            bbox: { x0, y0, x1, y1 },
            fontName: item.fontName || '',
            fontSize: item.height || 0,
            confidence: 1.0 // PDF text is highly reliable
          };
        });

        // Pre-compute structure to avoid token overload
        const structure = this.computeTextStructure(textItems, viewport);
        
        pages.push({
          page: pageNum,
          width: viewport.width,
          height: viewport.height,
          textItems,
          structure,
          fullText: textItems.map(item => item.text).join(' ')
        });
        
        totalTextItems += textItems.length;
      }

      return {
        pages,
        totalTextItems,
        totalPages: pdf.numPages,
        extractionQuality: 1.0, // PDF text extraction is highly reliable
        extractionMethod: 'pdf_native'
      };
      
    } catch (error) {
      console.error('PDF extraction failed:', error);
      throw error;
    }
  }

  // Image OCR with robust worker management (fallback for image files)
  private async extractImageTextWithOCR(fileBuffer: Buffer): Promise<any> {
    try {
      // Create dedicated worker for reliability
      const worker = await Tesseract.createWorker('eng');
      
      try {
        const { data } = await worker.recognize(fileBuffer);
        
        // Extract items with normalized coordinates from OCR data
        const width = (data as any).width || 1000;
        const height = (data as any).height || 1000;
        const words = (data as any).words || [];
        
        const textItems = words.map((word: any) => ({
          text: word.text || '',
          bbox: {
            x0: (word.bbox?.x0 || 0) / width,
            y0: (word.bbox?.y0 || 0) / height,
            x1: (word.bbox?.x1 || 0) / width,
            y1: (word.bbox?.y1 || 0) / height
          },
          confidence: (word.confidence || 0) / 100
        }));

        // Pre-compute structure
        const structure = this.computeTextStructure(textItems, { width, height });
        
        return {
          pages: [{
            page: 1,
            width,
            height,
            textItems,
            structure,
            fullText: data.text || ''
          }],
          totalTextItems: textItems.length,
          totalPages: 1,
          extractionQuality: textItems.length > 0 ? 
            textItems.reduce((sum: number, item: any) => sum + item.confidence, 0) / textItems.length : 0,
          extractionMethod: 'ocr_tesseract'
        };
      } finally {
        await worker.terminate();
      }
    } catch (error) {
      console.error('OCR extraction failed:', error);
      throw error;
    }
  }

  // Pre-compute text structure to avoid token overload in LLM calls
  private computeTextStructure(textItems: any[], viewport: any): any {
    if (!textItems.length) return { lines: [], blocks: [], tables: [] };
    
    // Group text items into lines based on Y coordinates
    const lines = this.groupIntoLines(textItems);
    
    // Group lines into blocks based on spacing and alignment
    const blocks = this.groupIntoBlocks(lines);
    
    // Detect potential table regions
    const tables = this.detectTableRegions(textItems);
    
    return {
      lines: lines.map(line => ({
        text: line.map(item => item.text).join(' '),
        bbox: this.computeBoundingBox(line),
        itemCount: line.length
      })),
      blocks: blocks.map(block => ({
        text: block.map(line => line.map(item => item.text).join(' ')).join('\n'),
        bbox: this.computeBoundingBox(block.flat()),
        lineCount: block.length
      })),
      tables: tables.map(table => ({
        bbox: table.bbox,
        estimatedRows: table.rows,
        estimatedCols: table.cols,
        confidence: table.confidence
      }))
    };
  }

  // Helper: Group text items into lines
  private groupIntoLines(textItems: any[]): any[][] {
    const lines: any[][] = [];
    const sorted = [...textItems].sort((a, b) => a.bbox.y0 - b.bbox.y0);
    
    let currentLine: any[] = [];
    let lastY = -1;
    const yThreshold = 0.02; // 2% of page height
    
    for (const item of sorted) {
      if (lastY < 0 || Math.abs(item.bbox.y0 - lastY) < yThreshold) {
        currentLine.push(item);
      } else {
        if (currentLine.length > 0) {
          currentLine.sort((a, b) => a.bbox.x0 - b.bbox.x0);
          lines.push(currentLine);
        }
        currentLine = [item];
      }
      lastY = item.bbox.y0;
    }
    
    if (currentLine.length > 0) {
      currentLine.sort((a, b) => a.bbox.x0 - b.bbox.x0);
      lines.push(currentLine);
    }
    
    return lines;
  }

  // Helper: Group lines into blocks
  private groupIntoBlocks(lines: any[][]): any[][][] {
    const blocks: any[][][] = [];
    let currentBlock: any[][] = [];
    let lastY = -1;
    const blockThreshold = 0.05; // 5% of page height
    
    for (const line of lines) {
      const lineY = line[0]?.bbox.y0 || 0;
      
      if (lastY < 0 || Math.abs(lineY - lastY) < blockThreshold) {
        currentBlock.push(line);
      } else {
        if (currentBlock.length > 0) {
          blocks.push(currentBlock);
        }
        currentBlock = [line];
      }
      lastY = lineY;
    }
    
    if (currentBlock.length > 0) {
      blocks.push(currentBlock);
    }
    
    return blocks;
  }

  // Helper: Detect table regions
  private detectTableRegions(textItems: any[]): any[] {
    // Simple table detection based on alignment patterns
    const tables: any[] = [];
    
    // Group items by approximate Y position
    const yGroups = new Map<number, any[]>();
    textItems.forEach(item => {
      const yKey = Math.round(item.bbox.y0 * 50) / 50; // Round to nearest 2%
      if (!yGroups.has(yKey)) yGroups.set(yKey, []);
      yGroups.get(yKey)!.push(item);
    });
    
    // Look for rows with consistent column alignment
    const rows = Array.from(yGroups.values()).filter(row => row.length >= 3);
    if (rows.length >= 3) {
      const bbox = this.computeBoundingBox(rows.flat());
      tables.push({
        bbox,
        rows: rows.length,
        cols: Math.max(...rows.map(row => row.length)),
        confidence: Math.min(1.0, rows.length / 10) // Higher confidence for more rows
      });
    }
    
    return tables;
  }

  // Helper: Compute bounding box for a group of items
  private computeBoundingBox(items: any[]): any {
    if (!items.length) return { x0: 0, y0: 0, x1: 0, y1: 0 };
    
    const x0 = Math.min(...items.map(item => item.bbox.x0));
    const y0 = Math.min(...items.map(item => item.bbox.y0));
    const x1 = Math.max(...items.map(item => item.bbox.x1));
    const y1 = Math.max(...items.map(item => item.bbox.y1));
    
    return { x0, y0, x1, y1 };
  }

  // PHASE 2: OPTIMIZED LAYOUT ANALYSIS (AVOIDING TOKEN OVERLOAD)
  private async performOptimizedLayoutAnalysis(openaiInstance: OpenAI, textData: any, options: ExtractionOptions): Promise<any> {
    try {
      const layoutPrompt = `üîç SMART LAYOUT ANALYSIS EXPERT

MISSION: Analyze document structure using pre-computed text blocks and structural hints.

DOCUMENT METRICS:
- Total Pages: ${textData.totalPages}
- Total Text Items: ${textData.totalTextItems}
- Extraction Method: ${textData.extractionMethod}

Analyze the provided STRUCTURED DATA (not raw coordinates) to identify layout patterns.

Return JSON with:
{
  "document_type": "invoice|report|form|article|table|mixed",
  "layout_structure": {
    "page_layout": "single_column|multi_column|table_heavy|form_based",
    "text_regions": [...],
    "table_regions": [...]
  },
  "quality_metrics": {
    "layout_clarity": 0.95,
    "text_organization": 0.90,
    "structure_complexity": "low|medium|high"
  }
}`;

      // Use pre-computed structure instead of raw coordinates to avoid token overload
      const structureSummary = textData.pages.map((page: any) => {
        return `PAGE ${page.page}:
Blocks: ${page.structure?.blocks?.length || 0}
Tables: ${page.structure?.tables?.length || 0}
Sample content: "${page.fullText.substring(0, 300)}..."
Structure summary: ${JSON.stringify({
  lines: page.structure?.lines?.length || 0,
  blocks: page.structure?.blocks?.length || 0,
  tables: page.structure?.tables?.length || 0
})}`;
      }).join('\n\n');

      const messages = [
        {
          role: "system" as const,
          content: layoutPrompt
        },
        {
          role: "user" as const,
          content: `Analyze this structured text data:\n\n${structureSummary}`
        }
      ];

      const response = await openaiInstance.chat.completions.create({
        model: "gpt-4o",
        messages,
        response_format: { type: "json_object" },
        max_tokens: 1024, // Reduced token usage
      });

      return JSON.parse(response.choices[0].message.content || '{}');
    } catch (error) {
      console.error('Optimized layout analysis failed:', error);
      return {
        document_type: "unknown",
        layout_structure: { page_layout: "single_column", text_regions: [], table_regions: [] },
        quality_metrics: { layout_clarity: 0.5, text_organization: 0.5, structure_complexity: "medium" }
      };
    }
  }

  // PHASE 3: CONTENT EXTRACTION WITH POSITIONING
  private async performPositionalExtraction(
    openaiInstance: OpenAI,
    ocrData: any,
    layoutAnalysis: any,
    options: ExtractionOptions
  ): Promise<any> {
    try {
      const systemPrompt = this.buildSystemPrompt(options);
      const enhancedPrompt = `${systemPrompt}

üß© ENRICHED CONTEXT FROM LAYOUT ANALYSIS:
Document Type: ${layoutAnalysis.document_type}
Layout: ${layoutAnalysis.layout_structure?.page_layout}
Identified Regions: ${layoutAnalysis.layout_structure?.text_regions?.length || 0} text regions, ${layoutAnalysis.layout_structure?.table_regions?.length || 0} table regions

üéØ ULTRA-PRECISION EXTRACTION WITH COORDINATES:
You have access to OCR data with exact word-level coordinates. Use this to create perfect extraction.

COORDINATE DATA AVAILABLE:
- ${ocrData.totalWords} words with precise bbox coordinates
- ${ocrData.totalPages} pages analyzed
- OCR Quality: ${(ocrData.ocrQuality * 100).toFixed(1)}%

Extract content using the coordinate data to ensure PERFECT positioning accuracy.`;

      // Prepare detailed OCR data for extraction
      const detailedOcrData = ocrData.pages.map((page: any) => {
        return `PAGE ${page.page}:
Full Text: "${page.text}"

WORDS WITH COORDINATES:
${page.words.map((w: any) => `"${w.text}" [${w.bbox.x0.toFixed(3)},${w.bbox.y0.toFixed(3)},${w.bbox.x1.toFixed(3)},${w.bbox.y1.toFixed(3)}] conf:${w.confidence.toFixed(2)}`).join('\n')}

LAYOUT CONTEXT:
${JSON.stringify(layoutAnalysis.layout_structure?.text_regions?.filter((r: any) => r.page === page.page) || [])}`;
      }).join('\n\n=== PAGE BREAK ===\n\n');

      const messages = [
        {
          role: "system" as const,
          content: enhancedPrompt
        },
        {
          role: "user" as const,
          content: `Extract content with PERFECT positioning using this coordinate data:\n\n${detailedOcrData}`
        }
      ];

      const response = await openaiInstance.chat.completions.create({
        model: "gpt-4o",
        messages,
        response_format: { type: "json_object" },
        max_tokens: 4096,
      });

      const result = JSON.parse(response.choices[0].message.content || '{}');
      return {
        ...result,
        confidence: ocrData.ocrQuality,
        processing_metadata: {
          ocr_words: ocrData.totalWords,
          ocr_quality: ocrData.ocrQuality,
          layout_type: layoutAnalysis.document_type,
          pages_processed: ocrData.totalPages
        }
      };
    } catch (error) {
      console.error('Positional extraction failed:', error);
      throw new Error(`Positional extraction failed: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  // PHASE 4: ADVANCED QUALITY VALIDATION
  private async performAdvancedQualityValidation(
    openaiInstance: OpenAI,
    contentExtraction: any,
    ocrData: any,
    options: ExtractionOptions
  ): Promise<ExtractionResult> {
    try {
      const validationPrompt = `üîç ULTRA-PRECISION QUALITY VALIDATOR

MISSION: Validate extraction completeness and accuracy against source OCR data.

SOURCE DATA METRICS:
- Total Words in OCR: ${ocrData.totalWords}
- OCR Quality: ${(ocrData.ocrQuality * 100).toFixed(1)}%
- Pages Processed: ${ocrData.totalPages}

VALIDATION CHECKLIST:
‚úÖ Content Completeness: Verify all visible text has been extracted
‚úÖ Positional Accuracy: Check coordinate precision and element positioning  
‚úÖ Structural Integrity: Ensure layout preservation and hierarchy
‚úÖ Format Preservation: Confirm text formatting and visual elements
‚úÖ Quality Metrics: Assess extraction confidence and reliability

Return enhanced/validated extraction with quality scores and any missing content identified.`;

      const extractionSummary = `EXTRACTION TO VALIDATE:
${JSON.stringify(contentExtraction, null, 2)}

EXTRACTION REFERENCE DATA:
Text Items: ${ocrData.totalTextItems}
Extraction Quality: ${(ocrData.extractionQuality * 100).toFixed(1)}%`;

      const messages = [
        {
          role: "system" as const,
          content: validationPrompt
        },
        {
          role: "user" as const,
          content: extractionSummary
        }
      ];

      const response = await openaiInstance.chat.completions.create({
        model: "gpt-4o",
        messages,
        response_format: { type: "json_object" },
        max_tokens: 4096,
      });

      const validatedResult = JSON.parse(response.choices[0].message.content || '{}');
      
      // Enhance with OCR metadata
      const enhancedResult = {
        ...validatedResult,
        metadata: {
          ...validatedResult.metadata,
          ocr_metrics: {
            total_words: ocrData.totalWords,
            ocr_quality: ocrData.ocrQuality,
            pages_processed: ocrData.totalPages
          },
          quality_metrics: {
            ...validatedResult.metadata?.quality_metrics,
            overall_confidence: Math.min(
              ocrData.ocrQuality * 100,
              validatedResult.metadata?.quality_metrics?.overall_confidence || 85
            ),
            extraction_method: "ultra_precision_multipass"
          }
        }
      };

      return this.formatResult(enhancedResult, options);
    } catch (error) {
      console.error('Quality validation failed:', error);
      // Fallback to basic formatting if validation fails
      return this.formatResult(contentExtraction, options);
    }
  }

  // ENHANCED FALLBACK METHOD
  private async performEnhancedSinglePass(
    openaiInstance: OpenAI,
    fileBuffer: Buffer,
    mimeType: string,
    options: ExtractionOptions
  ): Promise<ExtractionResult> {
    try {
      const systemPrompt = this.buildSystemPrompt(options);
      const messages = await this.buildAnalysisMessages(fileBuffer, mimeType, systemPrompt);
      
      const response = await openaiInstance.chat.completions.create({
        model: "gpt-4o",
        messages,
        response_format: { type: "json_object" },
        max_tokens: 4096,
      });

      const result = JSON.parse(response.choices[0].message.content || '{}');
      return this.formatResult(result, options);
    } catch (error) {
      throw new Error(`Enhanced single-pass extraction failed: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  // VLM LAYOUT-AWARE EXTRACTION METHOD
  private async performVLMLayoutAwareExtraction(
    openaiInstance: OpenAI,
    textData: any,
    options: ExtractionOptions
  ): Promise<ExtractionResult> {
    try {
      console.log(`üß† Starting VLM layout-aware extraction with semantic understanding`);
      
      // Convert text data to basic page format for VLM processing
      const basicPages = this.convertTextDataToPages(textData);
      
      // Apply VLM semantic lift processing
      const apiKey = options.openaiApiKey || process.env.OPENAI_API_KEY || process.env.OPENAI_KEY || "";
      const enhancedIR = await vlmSemanticEngine.performSemanticLift(basicPages, 'openai', apiKey);
      
      console.log(`üéØ VLM processing completed for ${enhancedIR.pages.length} pages with ${enhancedIR.documentMetrics.totalBlocks} blocks`);
      
      // Convert IR back to ExtractionResult format
      const extractionResult = this.convertIRToExtractionResult(enhancedIR, options);
      
      return extractionResult;
    } catch (error) {
      console.error('üîÑ VLM layout-aware extraction failed, falling back to standard extraction:', error);
      throw error;
    }
  }

  // Convert text data to basic pages format for VLM processing
  private convertTextDataToPages(textData: any): any[] {
    return textData.pages.map((pageData: any, index: number) => ({
      pageNumber: pageData.page || index + 1,
      width: pageData.width || 1000,
      height: pageData.height || 1000,
      words: pageData.textItems || [],
      lines: [],
      blocks: [],
      tables: [],
      shapes: [],
      coverage: {
        pdfNativeWords: pageData.textItems?.length || 0,
        ocrWords: 0,
        reconciledWords: pageData.textItems?.length || 0,
        coveragePercent: textData.extractionQuality * 100 || 85,
        missedWords: [],
      },
    }));
  }

  // Convert IR back to ExtractionResult format
  private convertIRToExtractionResult(ir: any, options: ExtractionOptions): ExtractionResult {
    // Extract tables from IR
    const tables = ir.pages.flatMap((page: any) => page.tables || []).map((table: any) => ({
      title: table.title,
      confidence: table.confidence,
      data: this.convertTableCellsToData(table.cells),
      headers: this.extractTableHeaders(table.cells),
      summary: {},
    }));

    // Extract key-value pairs as structured content
    const keyValuePairs = ir.pages.flatMap((page: any) => page.keyValuePairs || []);
    const structuredContent = this.formatKeyValuePairs(keyValuePairs);

    // Extract all text content
    const allText = ir.pages.map((page: any) => 
      page.blocks.map((block: any) => 
        block.lines.map((line: any) => 
          line.words.map((word: any) => word.text).join(' ')
        ).join('\n')
      ).join('\n\n')
    ).join('\n\n');

    return {
      tables,
      text: allText,
      structured_content: structuredContent,
      ir, // Include the full IR for advanced use cases
      metadata: {
        processed_at: new Date().toISOString(),
        ai_provider: 'openai',
        extraction_mode: 'vlm_layout_aware',
        page_count: ir.pages.length,
        word_count: ir.documentMetrics.totalWords,
        has_text: ir.documentMetrics.totalWords > 0,
        object_count: ir.documentMetrics.totalBlocks,
        coverage_metrics: {
          overall_coverage: ir.documentMetrics.overallCoverage,
          method_coverage: {
            vlm_semantic_lift: 100,
            layout_engine: 100,
            pdf_native: ir.pages[0]?.coverage?.coveragePercent || 0,
          },
          quality_score: Math.min(95, ir.documentMetrics.overallCoverage + 10),
        },
        processing_pipeline: ir.documentMetrics.extractionMethods,
      },
    };
  }

  // Helper methods for IR conversion
  private convertTableCellsToData(cells: any[]): any[] {
    const rowData: any[] = [];
    const maxRow = Math.max(...cells.map(c => c.row));
    
    for (let row = 0; row <= maxRow; row++) {
      const rowCells = cells.filter(c => c.row === row).sort((a, b) => a.col - b.col);
      const dataRow: any = {};
      rowCells.forEach(cell => {
        dataRow[`col_${cell.col}`] = cell.text;
      });
      rowData.push(dataRow);
    }
    
    return rowData;
  }

  private extractTableHeaders(cells: any[]): string[] {
    return cells
      .filter(c => c.isHeader || c.row === 0)
      .sort((a, b) => a.col - b.col)
      .map(c => c.text);
  }

  private formatKeyValuePairs(keyValuePairs: any[]): string {
    return keyValuePairs
      .map(kv => `${kv.key.text}: ${kv.value.text}`)
      .join('\n');
  }

  // Helper method to build messages for different analysis passes
  private async buildAnalysisMessages(fileBuffer: Buffer, mimeType: string, systemPrompt: string): Promise<any[]> {
    if (mimeType === 'application/pdf') {
      // For PDFs, extract text and analyze
      const pdfData = await pdfParse(fileBuffer);
      const extractedText = pdfData.text;
      
      if (!extractedText || extractedText.trim().length === 0) {
        throw new Error("Could not extract text from PDF. The PDF may be image-only or corrupted.");
      }

      return [
        {
          role: "system" as const,
          content: systemPrompt
        },
        {
          role: "user" as const,
          content: `Extract data from this PDF document according to the specified requirements. Focus on preserving table structure and maintaining data integrity with PERFECT ACCURACY.

PDF Content:
${extractedText}

Please analyze this content and generate the structured output as specified in the system prompt.`
        }
      ];
    } else {
      // For images, use vision capabilities
      const base64Image = fileBuffer.toString('base64');
      return [
        {
          role: "system" as const,
          content: systemPrompt
        },
        {
          role: "user" as const,
          content: [
            {
              type: "text" as const,
              text: "Extract data from this document according to the specified requirements. Focus on preserving table structure and maintaining data integrity with PERFECT ACCURACY."
            },
            {
              type: "image_url" as const,
              image_url: {
                url: `data:${mimeType};base64,${base64Image}`
              }
            }
          ],
        }
      ];
    }
  }

  private getMimeType(filePath: string): string {
    const ext = filePath.toLowerCase().split('.').pop();
    switch (ext) {
      case 'pdf': return 'application/pdf';
      case 'jpg':
      case 'jpeg': return 'image/jpeg';
      case 'png': return 'image/png';
      default: return 'application/octet-stream';
    }
  }

  private buildSystemPrompt(options: ExtractionOptions): string {
    const basePrompt = `You are an EXPERT PRECISION DOCUMENT EXTRACTION SYSTEM with ADVANCED OCR, LAYOUT ANALYSIS, and DOCUMENT UNDERSTANDING capabilities. 

üéØ CRITICAL MISSION: PERFECT EXTRACTION WITH ZERO LOSS
- Extract EVERY SINGLE CHARACTER, SYMBOL, LINE, SPACE, and FORMATTING ELEMENT
- Preserve EXACT LAYOUT: where lines exist, maintain lines; where spaces exist, maintain spaces
- Capture PRECISE POSITIONING and STRUCTURAL RELATIONSHIPS
- Maintain ORIGINAL FORMATTING including indentation, alignment, and visual hierarchy
- NO WORD, CHARACTER, OR FORMATTING ELEMENT SHOULD BE MISSED OR MODIFIED

üìê LAYOUT PRESERVATION REQUIREMENTS:
- Preserve exact line breaks, spacing, and indentation
- Maintain table borders, cell alignment, and column structures exactly as shown
- Keep heading hierarchies and text formatting (bold, italic, underline) intact
- Preserve bullet points, numbering, and list structures exactly
- Maintain page breaks, sections, and document flow
- Capture margins, padding, and white space relationships`;
    
    let modePrompt = "";
    switch (options.extractionMode) {
      case 'smart_table':
        modePrompt = `üîç ADVANCED TABLE EXTRACTION MODE:
        - Detect ALL tabular structures including nested tables, merged cells, and complex layouts
        - Preserve EXACT cell boundaries, borders, and alignment
        - Maintain header-data relationships and table hierarchy
        - Capture table titles, captions, and footnotes with precise positioning
        - Extract cell formatting: borders, shading, text alignment, font styles
        - Preserve table-to-text relationships and surrounding context
        - Identify and extract table metadata: column types, data patterns, relationships`;
        break;
      case 'full_text':
        modePrompt = `üìñ COMPREHENSIVE TEXT EXTRACTION MODE:
        - Extract ALL textual content: headers, paragraphs, footnotes, captions, annotations
        - Preserve EXACT paragraph structure, line breaks, and spacing
        - Maintain text hierarchy: titles, subtitles, headers (H1-H6), body text
        - Capture text formatting: bold, italic, underline, font changes, size variations
        - Preserve lists: numbered, bulleted, nested lists with exact indentation
        - Extract marginalia, sidebars, callouts, and text boxes with positioning
        - Maintain reading order and document flow exactly as presented
        - Capture text-to-element relationships (captions to figures, etc.)`;
        break;
      case 'form_fields':
        modePrompt = `üìù PRECISION FORM FIELD EXTRACTION MODE:
        - Identify ALL form elements: input fields, checkboxes, radio buttons, dropdowns
        - Extract field labels, placeholders, and associated text exactly as positioned
        - Capture field values, default states, and validation requirements
        - Preserve form layout: field alignment, groupings, and visual relationships
        - Extract form structure: sections, pages, field dependencies
        - Maintain label-to-field associations and spatial relationships
        - Capture form metadata: field types, constraints, required/optional status`;
        break;
      case 'custom':
        modePrompt = `üéõÔ∏è CUSTOM SCHEMA EXTRACTION MODE:
        - Extract data according to the provided custom schema with PERFECT ACCURACY
        - Maintain exact positioning and structural relationships as specified
        - Preserve all formatting and layout elements defined in the schema
        - Capture additional context and metadata beyond the schema requirements
        - Ensure schema compliance while maintaining document fidelity`;
        break;
      case 'smart_image':
        modePrompt = `üñºÔ∏è ADVANCED IMAGE ANALYSIS MODE:
        - Identify and catalog ALL visible elements: objects, people, text, symbols, graphics
        - Extract ANY visible text using advanced OCR with exact positioning
        - Analyze spatial relationships between elements
        - Provide detailed descriptions maintaining visual hierarchy
        - Capture image quality, resolution, and technical metadata
        - Preserve context and relationships between visual elements`;
        break;
      case 'vlm_layout_aware':
        modePrompt = `üß† VLM LAYOUT-AWARE DOCUMENT UNDERSTANDING MODE:
        - Apply Vision-Language Model intelligence for comprehensive document understanding
        - Analyze textual content, visual features, and spatial relationships simultaneously
        - Preserve exact layout with normalized coordinate systems and spatial encoding
        - Understand document hierarchy and semantic structure with reading order
        - Extract key-value pairs with spatial relationships and semantic types
        - Perform layout-aware processing with enhanced table structure analysis
        - Maintain bounding box relationships and document flow understanding
        - Apply semantic lift for business context and document type classification
        - Generate comprehensive intermediate representation with spatial graph analysis`;
        break;
    }

    const formatPrompt = options.preserveFormatting 
      ? `üé® ULTRA-PRECISE FORMATTING PRESERVATION:
      - Maintain EXACT character-level spacing, indentation, and alignment
      - Preserve all visual formatting: font styles, sizes, colors, emphasis
      - Capture layout elements: borders, lines, boxes, dividers, backgrounds
      - Maintain spatial relationships: distances, margins, padding, positioning
      - Preserve document structure: sections, columns, blocks, hierarchies
      - Extract with pixel-level coordinate precision for positioning data`
      : `üìä CONTENT-FOCUSED EXTRACTION WITH POSITIONING:
      - Extract core content while preserving essential structural elements
      - Maintain logical hierarchy and document flow
      - Include positional data for spatial context and relationships
      - Preserve key formatting that affects meaning and interpretation`;

    const confidencePrompt = options.includeConfidence
      ? `üìà DETAILED CONFIDENCE ANALYSIS:
      - Provide granular confidence scores (0.0-1.0) for each extracted element
      - Include extraction certainty metrics for text recognition accuracy
      - Assess layout detection confidence and positioning precision
      - Evaluate structural analysis confidence and relationship accuracy
      - Provide overall document processing confidence and quality metrics`
      : `üìà CONFIDENCE ASSESSMENT:
      - Include reasonable confidence estimates for all extracted elements
      - Assess overall extraction quality and accuracy
      - Highlight areas of uncertainty or potential extraction challenges`;

    const structurePrompt = options.extractionMode === 'smart_image' 
      ? `Return the extracted data as JSON with the following structure:
    {
      "objects": [
        {
          "label": "string (name of the object)",
          "confidence": number (0-1),
          "description": "string (optional detailed description)",
          "category": "string (optional: person, animal, vehicle, furniture, etc.)"
        }
      ],
      "text": "string (optional if any text is found)",
      "metadata": {
        "processed_at": "ISO timestamp",
        "ai_provider": "openai",
        "extraction_mode": "${options.extractionMode}",
        "has_text": boolean,
        "object_count": number
      }
    }`
      : `üéØ ULTRA-PRECISION JSON OUTPUT FORMAT:
    {
      "structured_content": "PIXEL-PERFECT MARKDOWN RECONSTRUCTION:\n\nüîπ EXACT LAYOUT PRESERVATION:\n- Recreate the document with IDENTICAL formatting, spacing, and structure\n- Use precise markdown syntax to mirror original appearance\n- Include ALL formatting: **bold**, *italic*, ~~strikethrough~~, \`code\`, headings (#, ##, ###)\n- Preserve EXACT line breaks, paragraph spacing, and indentation\n- Maintain original text alignment and visual hierarchy\n\nüîπ POSITIONAL METADATA (for every element):\n<!-- ELEMENT_TYPE: text/table/figure/list/heading | PAGE: X | BBOX: l=0.123,t=0.456,r=0.789,b=0.901 | ID: uuid-here | CONFIDENCE: 0.95 -->\n\nüîπ STRUCTURAL ELEMENTS:\n- Headings: Use proper markdown levels (# ## ###) matching original hierarchy\n- Tables: Preserve exact cell content, alignment, borders using markdown tables\n- Lists: Maintain numbering/bullet style and nested indentation\n- Text blocks: Preserve paragraph breaks and formatting\n- Special elements: Quotes, code blocks, callouts with original styling\n\nüîπ CONTENT REQUIREMENTS:\n- Extract EVERY character, symbol, and space exactly as shown\n- Preserve all text formatting and emphasis\n- Maintain spatial relationships between elements\n- Include document flow and reading order\n- Capture marginalia, footnotes, headers, footers\n\nüîπ EXAMPLE FORMAT:\n<!-- ELEMENT_TYPE: heading | PAGE: 1 | BBOX: l=0.1,t=0.2,r=0.9,b=0.25 | ID: h1-abc123 | CONFIDENCE: 0.98 -->\n# Main Document Title\n\n<!-- ELEMENT_TYPE: paragraph | PAGE: 1 | BBOX: l=0.1,t=0.3,r=0.9,b=0.4 | ID: p1-def456 | CONFIDENCE: 0.97 -->\nThis is the exact paragraph text with **bold formatting** and *italic text* preserved exactly as shown in the original document.\n\n<!-- ELEMENT_TYPE: table | PAGE: 1 | BBOX: l=0.1,t=0.5,r=0.9,b=0.7 | ID: table-ghi789 | CONFIDENCE: 0.99 -->\n| Column 1 | Column 2 | Column 3 |\n|----------|----------|----------|\n| Data 1   | Data 2   | Data 3   |",
      "tables": [
        {
          "title": "Exact table title/caption as shown",
          "confidence": 0.95,
          "data": [{"Column_Name": "Exact_Cell_Content", "Another_Col": "Precise_Value"}],
          "headers": ["Exact column headers maintaining original formatting"],
          "positioning": {
            "page": 1,
            "bbox": {"left": 0.1, "top": 0.3, "right": 0.9, "bottom": 0.6},
            "cell_boundaries": [{"row": 0, "col": 0, "bbox": {"left": 0.1, "top": 0.3, "right": 0.5, "bottom": 0.4}}]
          },
          "formatting": {
            "border_style": "solid|dashed|dotted",
            "header_formatting": ["bold", "italic"],
            "cell_alignment": ["left", "center", "right"],
            "background_colors": ["#ffffff", "#f0f0f0"]
          },
          "structure_analysis": {
            "has_merged_cells": false,
            "nested_tables": false,
            "table_type": "data|form|layout",
            "relationships": "description of how this table relates to surrounding content"
          }
        }
      ],
      "text_blocks": [
        {
          "content": "Exact text content with original formatting preserved",
          "type": "paragraph|heading|list|caption|footnote|marginalia",
          "formatting": ["bold", "italic", "underline"],
          "hierarchy_level": 1,
          "positioning": {
            "page": 1,
            "bbox": {"left": 0.1, "top": 0.1, "right": 0.9, "bottom": 0.2}
          },
          "confidence": 0.98
        }
      ],
      "layout_elements": [
        {
          "type": "line|box|border|separator|image|logo",
          "description": "Visual element description",
          "positioning": {
            "page": 1,
            "bbox": {"left": 0.0, "top": 0.0, "right": 1.0, "bottom": 0.05}
          }
        }
      ],
      "metadata": {
        "processed_at": "${new Date().toISOString()}",
        "ai_provider": "openai",
        "extraction_mode": "${options.extractionMode}",
        "document_analysis": {
          "page_count": 1,
          "word_count": 0,
          "character_count": 0,
          "line_count": 0,
          "paragraph_count": 0,
          "table_count": 0,
          "image_count": 0,
          "heading_count": 0,
          "list_count": 0
        },
        "quality_metrics": {
          "overall_confidence": 0.95,
          "text_clarity": 0.98,
          "layout_detection": 0.94,
          "formatting_preservation": 0.96,
          "completeness_score": 0.99
        },
        "extraction_challenges": [],
        "processing_notes": "Any important observations about the document structure or extraction process"
      }
    }

üö® CRITICAL EXTRACTION REQUIREMENTS:
1. ZERO TOLERANCE FOR MISSING CONTENT - Extract every character, symbol, space, and formatting element
2. PERFECT LAYOUT RECONSTRUCTION - Maintain exact positioning, alignment, and visual hierarchy
3. COMPREHENSIVE POSITIONAL DATA - Include pixel-accurate coordinates for every element
4. FORMAT PRESERVATION - Keep original text formatting, font styles, and emphasis exactly as shown
5. STRUCTURAL INTEGRITY - Preserve document flow, reading order, and element relationships
6. QUALITY VALIDATION - Ensure extraction completeness and accuracy before finalizing
7. METADATA RICHNESS - Provide detailed analysis and confidence metrics for every element

‚ö° MANDATORY OUTPUT QUALITY CHECKS:
- Verify every visible text element has been extracted
- Confirm all tables, lists, and structured data are captured
- Validate formatting preservation and layout accuracy  
- Ensure positional coordinates are precise and complete
- Check that no content has been summarized, paraphrased, or modified
- Confirm original document structure is perfectly reconstructed

üéØ SUCCESS CRITERIA: The extracted markdown should be indistinguishable from the original document in terms of content, structure, and visual presentation when rendered.`;

    return `${basePrompt}\n\n${modePrompt}\n\n${formatPrompt}\n\n${confidencePrompt}\n\n${structurePrompt}`;
  }

  private formatResult(result: any, options: ExtractionOptions): ExtractionResult {
    return {
      tables: result.tables || [],
      text: result.text,
      objects: result.objects || undefined,
      structured_content: result.structured_content,
      metadata: {
        processed_at: new Date().toISOString(),
        ai_provider: "openai",
        extraction_mode: options.extractionMode,
        page_count: result.metadata?.page_count || 1,
        word_count: result.metadata?.word_count || 0,
        has_text: result.metadata?.has_text,
        object_count: result.metadata?.object_count,
      }
    };
  }
}

class GeminiProvider implements AIProvider {
  async extractData(filePath: string, options: ExtractionOptions, apiKey?: string): Promise<ExtractionResult> {
    try {
      console.log(`üöÄ Starting Google Gemini advanced extraction for: ${filePath}, mode: ${options.extractionMode}`);
      
      const fileBuffer = fs.readFileSync(filePath);
      const mimeType = this.getMimeType(filePath);

      // Use provided API key or fallback to environment variables (restored full chain)
      const effectiveApiKey = apiKey || options.geminiApiKey || process.env.GOOGLE_GEMINI_API_KEY || process.env.GEMINI_API_KEY || process.env.GOOGLE_API_KEY || process.env.GOOGLE_AI_KEY || "";
      
      if (!effectiveApiKey) {
        throw new Error("Gemini API key is required. Please provide an API key or set one of: GOOGLE_GEMINI_API_KEY, GEMINI_API_KEY, GOOGLE_API_KEY, or GOOGLE_AI_KEY environment variable.");
      }
      
      const geminiInstance = new GoogleGenerativeAI(effectiveApiKey);

      // ADVANCED MULTI-PASS EXTRACTION PIPELINE using Gemini's superior capabilities
      return await this.performAdvancedGeminiExtraction(geminiInstance, filePath, fileBuffer, mimeType, options);
    } catch (error) {
      throw new Error(`Google Gemini extraction failed: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  // ADVANCED MULTI-PASS EXTRACTION METHOD leveraging Gemini's superior document understanding
  private async performAdvancedGeminiExtraction(
    geminiInstance: GoogleGenerativeAI, 
    filePath: string, 
    fileBuffer: Buffer, 
    mimeType: string, 
    options: ExtractionOptions
  ): Promise<ExtractionResult> {
    try {
      console.log(`üìä Gemini processing: ${mimeType} document with ${options.extractionMode} mode`);
      
      // Use Gemini 2.0 Flash for advanced document understanding
      const model = geminiInstance.getGenerativeModel({ 
        model: "gemini-2.0-flash",
        generationConfig: {
          temperature: 0.1, // Low temperature for precise extraction
          maxOutputTokens: 8192
        }
      });

      // Prepare content based on document type
      const contents = await this.prepareDocumentContents(fileBuffer, mimeType);
      
      // Execute extraction based on mode
      switch (options.extractionMode) {
        case 'smart_table':
          return await this.performSmartTableDetection(model, contents, options);
        case 'full_text':
          return await this.performFullTextExtraction(model, contents, options);
        case 'form_fields':
          return await this.performFormFieldDetection(model, contents, options);
        case 'smart_image':
          return await this.performSmartImageObjectDetection(model, contents, options);
        case 'automatic_schema':
          return await this.performAutomaticSchemaGeneration(model, contents, options);
        case 'vlm_layout_aware':
          return await this.performVLMLayoutAwareExtraction(geminiInstance, fileBuffer, mimeType, options);
        case 'custom':
          return await this.performCustomExtraction(model, contents, options);
        case 'comprehensive':
          return await this.performComprehensiveExtraction(model, contents, options);
        default:
          return await this.performComprehensiveExtraction(model, contents, options);
      }
    } catch (error) {
      console.error('Advanced Gemini extraction failed:', error);
      // Fallback to basic extraction
      return await this.performBasicGeminiExtraction(geminiInstance, fileBuffer, mimeType, options);
    }
  }

  // Prepare document contents for Gemini processing
  private async prepareDocumentContents(fileBuffer: Buffer, mimeType: string): Promise<any[]> {
    if (mimeType === 'application/pdf') {
      // For PDFs, use Gemini's native PDF processing capabilities
      return [
        {
          inlineData: {
            data: fileBuffer.toString('base64'),
            mimeType: 'application/pdf',
          },
        }
      ];
    } else {
      // For images, use Gemini's vision capabilities
      return [
        {
          inlineData: {
            data: fileBuffer.toString('base64'),
            mimeType: mimeType,
          },
        }
      ];
    }
  }

  // SMART TABLE DETECTION - Superior to Landing AI
  private async performSmartTableDetection(model: any, contents: any[], options: ExtractionOptions): Promise<ExtractionResult> {
    const prompt = `üîç ADVANCED TABLE DETECTION & EXTRACTION EXPERT

You are a specialized table detection and extraction system using Google Gemini's superior document understanding capabilities.

MISSION: Detect, analyze, and extract ALL tables from this document with perfect accuracy and structure preservation.

CAPABILITIES:
‚úÖ Smart table boundary detection
‚úÖ Header identification and classification
‚úÖ Cell content extraction with data type recognition
‚úÖ Table relationship analysis
‚úÖ Multi-page table handling
‚úÖ Complex table structure support (merged cells, nested tables)

EXTRACTION REQUIREMENTS:
1. Detect ALL tables in the document, regardless of format or complexity
2. Preserve exact table structure including headers, rows, and columns
3. Extract cell content with proper data typing (numbers, text, dates)
4. Identify table relationships and dependencies
5. Provide confidence scores for each detected table
6. Include positional information and table metadata

OUTPUT FORMAT: Return valid JSON with comprehensive table data:
{
  "tables": [
    {
      "title": "Table title or caption",
      "confidence": 0.98,
      "page": 1,
      "position": {"x": 0.1, "y": 0.2, "width": 0.8, "height": 0.3},
      "headers": ["Column 1", "Column 2", "Column 3"],
      "data": [
        {"Column 1": "Value1", "Column 2": 123.45, "Column 3": "2024-01-01"},
        {"Column 1": "Value2", "Column 2": 678.90, "Column 3": "2024-01-02"}
      ],
      "metadata": {
        "rows": 2,
        "columns": 3,
        "has_headers": true,
        "table_type": "data_table",
        "complexity": "simple"
      }
    }
  ],
  "text": "Any additional text found outside tables",
  "metadata": {
    "processed_at": "${new Date().toISOString()}",
    "ai_provider": "gemini",
    "extraction_mode": "smart_table",
    "table_count": 1,
    "total_cells": 6,
    "processing_confidence": 0.95
  }
}

Analyze the document and extract ALL tables with maximum precision.`;

    const response = await model.generateContent([prompt, ...contents]);
    return this.parseAndFormatResponse(response, options);
  }

  // FORM FIELD DETECTION - Advanced field recognition
  private async performFormFieldDetection(model: any, contents: any[], options: ExtractionOptions): Promise<ExtractionResult> {
    const prompt = `üìù ADVANCED FORM FIELD DETECTION & EXTRACTION EXPERT

You are a specialized form analysis system using Google Gemini's superior document understanding.

MISSION: Detect, analyze, and extract ALL form fields with their labels, values, and relationships.

CAPABILITIES:
‚úÖ Form field boundary detection
‚úÖ Label-value pair identification
‚úÖ Field type classification (text, number, date, checkbox, dropdown)
‚úÖ Required field detection
‚úÖ Form validation rules extraction
‚úÖ Multi-section form handling

EXTRACTION REQUIREMENTS:
1. Identify ALL form fields including text inputs, checkboxes, radio buttons, dropdowns
2. Extract field labels and their corresponding values
3. Determine field types and validation requirements
4. Identify form sections and groupings
5. Provide confidence scores for each field detection
6. Include positional and relationship information

OUTPUT FORMAT: Return valid JSON with comprehensive form data:
{
  "form_fields": [
    {
      "label": "Full Name",
      "value": "John Smith",
      "field_type": "text",
      "required": true,
      "confidence": 0.97,
      "position": {"x": 0.1, "y": 0.1, "width": 0.3, "height": 0.05},
      "section": "Personal Information"
    }
  ],
  "form_structure": {
    "sections": ["Personal Information", "Contact Details", "Preferences"],
    "total_fields": 15,
    "required_fields": 8,
    "form_type": "application_form"
  },
  "text": "Any additional text content",
  "metadata": {
    "processed_at": "${new Date().toISOString()}",
    "ai_provider": "gemini",
    "extraction_mode": "form_fields",
    "field_count": 15,
    "completion_rate": 0.87
  }
}

Analyze the document and extract ALL form fields with maximum accuracy.`;

    const response = await model.generateContent([prompt, ...contents]);
    return this.parseAndFormatResponse(response, options);
  }

  // SMART IMAGE OBJECT DETECTION - Advanced visual analysis
  private async performSmartImageObjectDetection(model: any, contents: any[], options: ExtractionOptions): Promise<ExtractionResult> {
    const prompt = `üëÅÔ∏è ADVANCED IMAGE OBJECT DETECTION & ANALYSIS EXPERT

You are a specialized visual analysis system using Google Gemini's superior computer vision capabilities.

MISSION: Detect, identify, and analyze ALL visible objects, people, text, and elements in the image/document.

CAPABILITIES:
‚úÖ Object detection and classification
‚úÖ People and face recognition
‚úÖ Text detection and OCR
‚úÖ Scene understanding and context analysis
‚úÖ Brand and logo recognition
‚úÖ Document element identification

DETECTION CATEGORIES:
- People (faces, clothing, gestures, activities)
- Vehicles (cars, trucks, bikes, planes, boats)
- Animals (pets, wildlife, livestock)
- Objects (furniture, electronics, tools, food)
- Buildings and Architecture
- Natural Elements (trees, mountains, water)
- Text and Signs
- Brands and Logos
- Documents and Papers

OUTPUT FORMAT: Return valid JSON with comprehensive object data:
{
  "objects": [
    {
      "label": "person",
      "confidence": 0.95,
      "description": "Person wearing blue shirt standing near desk",
      "category": "people",
      "position": {"x": 0.2, "y": 0.1, "width": 0.3, "height": 0.6},
      "attributes": ["blue_shirt", "standing", "male"]
    }
  ],
  "text": "All detected text content in the image",
  "scene_analysis": {
    "setting": "office environment",
    "primary_subjects": ["person", "desk", "computer"],
    "mood": "professional",
    "lighting": "indoor_artificial"
  },
  "metadata": {
    "processed_at": "${new Date().toISOString()}",
    "ai_provider": "gemini",
    "extraction_mode": "smart_image",
    "object_count": 12,
    "text_detected": true,
    "analysis_confidence": 0.93
  }
}

Analyze the image and detect ALL visible objects, people, text, and elements with maximum detail.`;

    const response = await model.generateContent([prompt, ...contents]);
    return this.parseAndFormatResponse(response, options);
  }

  // VLM LAYOUT-AWARE EXTRACTION using shared semantic engine (consistent with OpenAI)
  private async performVLMLayoutAwareExtraction(
    geminiInstance: GoogleGenerativeAI,
    fileBuffer: Buffer,
    mimeType: string,
    options: ExtractionOptions
  ): Promise<ExtractionResult> {
    try {
      console.log(`üß† Starting VLM layout-aware extraction with semantic understanding using Gemini`);
      
      // For VLM processing, use basic extraction approach
      // For now, fall back to automatic schema generation until VLM engine supports Gemini
      const model = geminiInstance.getGenerativeModel({ model: "gemini-2.0-flash" });
      const contents = await this.prepareDocumentContents(fileBuffer, mimeType);
      const result = await this.performAutomaticSchemaGeneration(model, contents, options);
      
      return this.validateAndNormalizeResult(result, options);
    } catch (error) {
      console.error('üîÑ VLM layout-aware extraction failed, falling back to automatic schema generation:', error);
      const model = geminiInstance.getGenerativeModel({ model: "gemini-2.0-flash" });
      const contents = await this.prepareDocumentContents(fileBuffer, mimeType);
      return await this.performAutomaticSchemaGeneration(model, contents, options);
    }
  }

  // AUTOMATIC SCHEMA GENERATION - Fallback for VLM when semantic engine fails  
  private async performAutomaticSchemaGeneration(model: any, contents: any[], options: ExtractionOptions): Promise<ExtractionResult> {
    const prompt = `üß† AUTOMATIC SCHEMA GENERATION & LAYOUT ANALYSIS EXPERT

You are an advanced document understanding system using Vision-Language Model capabilities for automatic schema generation.

MISSION: Analyze document structure and automatically generate extraction schemas based on document layout and content patterns.

CAPABILITIES:
‚úÖ Document structure analysis
‚úÖ Content pattern recognition
‚úÖ Automatic field detection
‚úÖ Schema generation based on document type
‚úÖ Layout-aware element positioning
‚úÖ Semantic relationship mapping

ANALYSIS PROCESS:
1. Analyze overall document layout and structure
2. Identify repeating patterns and field types
3. Generate appropriate extraction schema
4. Extract data using the generated schema
5. Provide schema recommendations for future use

OUTPUT FORMAT: Return valid JSON with schema and extracted data:
{
  "generated_schema": {
    "document_type": "invoice",
    "fields": [
      {"name": "invoice_number", "type": "string", "required": true, "pattern": "INV-\\d+"},
      {"name": "date", "type": "date", "required": true},
      {"name": "total_amount", "type": "number", "required": true}
    ],
    "tables": [
      {"name": "line_items", "columns": ["description", "quantity", "unit_price", "total"]}
    ]
  },
  "extracted_data": {
    "invoice_number": "INV-12345",
    "date": "2024-09-21",
    "total_amount": 1234.56,
    "line_items": [
      {"description": "Product A", "quantity": 2, "unit_price": 100.00, "total": 200.00}
    ]
  },
  "layout_analysis": {
    "regions": [
      {"type": "header", "position": {"x": 0, "y": 0, "width": 1, "height": 0.2}},
      {"type": "content", "position": {"x": 0, "y": 0.2, "width": 1, "height": 0.6}},
      {"type": "footer", "position": {"x": 0, "y": 0.8, "width": 1, "height": 0.2}}
    ]
  },
  "metadata": {
    "processed_at": "${new Date().toISOString()}",
    "ai_provider": "gemini",
    "extraction_mode": "vlm_layout_aware",
    "schema_confidence": 0.91,
    "reusable_schema": true
  }
}

Analyze the document and generate an optimal extraction schema automatically.`;

    const response = await model.generateContent([prompt, ...contents]);
    return this.parseAndFormatResponse(response, options);
  }

  // FULL TEXT EXTRACTION with enhanced layout understanding
  private async performFullTextExtraction(model: any, contents: any[], options: ExtractionOptions): Promise<ExtractionResult> {
    const prompt = `üìÑ ADVANCED FULL TEXT EXTRACTION & LAYOUT PRESERVATION EXPERT

You are a comprehensive text extraction system using Google Gemini's superior document understanding.

MISSION: Extract ALL text content while preserving layout, formatting, and document structure.

CAPABILITIES:
‚úÖ Complete text extraction with layout preservation
‚úÖ Hierarchical structure detection (headings, paragraphs, lists)
‚úÖ Text formatting preservation (bold, italic, underline)
‚úÖ Reading order determination
‚úÖ Text block classification and positioning

EXTRACTION REQUIREMENTS:
1. Extract ALL visible text content from the document
2. Preserve original formatting and layout structure
3. Maintain proper reading order and hierarchy
4. Identify text blocks, headings, paragraphs, and lists
5. Include positional information for layout reconstruction
6. Provide confidence scores for text recognition

OUTPUT FORMAT: Return valid JSON with comprehensive text data:
{
  "structured_content": "# Document Title\\n\\nThis is the full extracted text with markdown formatting preserved...\\n\\n## Section Heading\\n\\nParagraph text here...",
  "text_blocks": [
    {
      "text": "Document Title",
      "type": "heading",
      "level": 1,
      "position": {"x": 0.1, "y": 0.05, "width": 0.8, "height": 0.08},
      "confidence": 0.98
    }
  ],
  "text": "Complete plain text content of the entire document",
  "document_structure": {
    "sections": ["Introduction", "Main Content", "Conclusion"],
    "total_paragraphs": 15,
    "total_headings": 5,
    "reading_order": ["title", "intro", "content", "conclusion"]
  },
  "metadata": {
    "processed_at": "${new Date().toISOString()}",
    "ai_provider": "gemini",
    "extraction_mode": "full_text",
    "word_count": 1250,
    "character_count": 7500,
    "extraction_confidence": 0.96
  }
}

Extract ALL text content with perfect layout and structure preservation.`;

    const response = await model.generateContent([prompt, ...contents]);
    return this.parseAndFormatResponse(response, options);
  }

  // CUSTOM EXTRACTION based on user-defined schema
  private async performCustomExtraction(model: any, contents: any[], options: ExtractionOptions): Promise<ExtractionResult> {
    const customSchema = options.customSchema ? JSON.stringify(options.customSchema, null, 2) : "No custom schema provided";
    
    const prompt = `‚öôÔ∏è CUSTOM SCHEMA EXTRACTION EXPERT

You are a flexible extraction system that adapts to user-defined schemas and requirements.

MISSION: Extract data according to the provided custom schema with maximum accuracy and completeness.

CUSTOM SCHEMA:
${customSchema}

EXTRACTION REQUIREMENTS:
1. Follow the provided schema exactly
2. Extract all requested fields and data structures
3. Maintain data types and validation rules specified in schema
4. Provide confidence scores for each extracted element
5. Handle missing or unclear data gracefully

OUTPUT FORMAT: Return valid JSON matching the custom schema structure with metadata:
{
  "extracted_data": {
    // Data structure matching the custom schema
  },
  "metadata": {
    "processed_at": "${new Date().toISOString()}",
    "ai_provider": "gemini",
    "extraction_mode": "custom",
    "schema_compliance": 0.95,
    "fields_extracted": 12,
    "fields_missing": 1
  }
}

Extract data according to the custom schema with maximum precision.`;

    const response = await model.generateContent([prompt, ...contents]);
    return this.parseAndFormatResponse(response, options);
  }

  // COMPREHENSIVE EXTRACTION - All features combined
  private async performComprehensiveExtraction(model: any, contents: any[], options: ExtractionOptions): Promise<ExtractionResult> {
    const prompt = `üåü COMPREHENSIVE DOCUMENT ANALYSIS & EXTRACTION EXPERT

You are an all-in-one document analysis system combining ALL advanced extraction capabilities.

MISSION: Perform complete document analysis extracting tables, text, objects, forms, and generating schemas automatically.

COMPREHENSIVE CAPABILITIES:
‚úÖ Smart table detection and extraction
‚úÖ Complete text extraction with layout preservation
‚úÖ Form field detection and analysis
‚úÖ Image object detection and analysis
‚úÖ Automatic schema generation
‚úÖ Document classification and understanding

EXTRACTION REQUIREMENTS:
1. Detect and extract ALL tables with structure preservation
2. Extract ALL text content with formatting and layout
3. Identify ALL form fields and their relationships
4. Detect ALL objects, people, and visual elements
5. Generate automatic extraction schemas
6. Provide comprehensive document analysis

OUTPUT FORMAT: Return complete JSON with all extraction types:
{
  "tables": [/* All detected tables */],
  "form_fields": [/* All form fields */],
  "objects": [/* All detected objects */],
  "text": "Complete text content",
  "structured_content": "Formatted markdown content",
  "generated_schema": {/* Auto-generated schema */},
  "document_analysis": {
    "document_type": "form_with_tables",
    "complexity": "high",
    "key_sections": ["header", "form_section", "data_tables", "signatures"],
    "processing_summary": "Document contains mixed content types..."
  },
  "metadata": {
    "processed_at": "${new Date().toISOString()}",
    "ai_provider": "gemini",
    "extraction_mode": "comprehensive",
    "overall_confidence": 0.94,
    "features_detected": ["tables", "forms", "text", "objects"]
  }
}

Perform comprehensive analysis and extract ALL content types with maximum detail.`;

    const response = await model.generateContent([prompt, ...contents]);
    return this.parseAndFormatResponse(response, options);
  }

  // Fallback basic extraction method
  private async performBasicGeminiExtraction(
    geminiInstance: GoogleGenerativeAI,
    fileBuffer: Buffer,
    mimeType: string,
    options: ExtractionOptions
  ): Promise<ExtractionResult> {
    const model = geminiInstance.getGenerativeModel({ model: "gemini-2.0-flash" });
    const systemPrompt = this.buildSystemPrompt(options);

    let contents;
    if (mimeType === 'application/pdf') {
      const base64Data = fileBuffer.toString('base64');
      contents = [{ inlineData: { data: base64Data, mimeType: 'application/pdf' } }];
    } else {
      const base64Data = fileBuffer.toString('base64');
      contents = [{ inlineData: { data: base64Data, mimeType: mimeType } }];
    }

    const response = await model.generateContent([systemPrompt, ...contents]);
    return this.parseAndFormatResponse(response, options);
  }

  // Schema validation and normalization (addressing architect feedback)
  private validateAndNormalizeResult(result: ExtractionResult, options: ExtractionOptions): ExtractionResult {
    try {
      // Validate using the extractionResultSchema
      const validationResult = extractionResultSchema.safeParse(result);
      
      if (!validationResult.success) {
        console.warn('Schema validation failed, normalizing result:', validationResult.error);
        
        // Normalize to ensure basic structure exists
        const normalizedResult: ExtractionResult = {
          tables: result.tables || [],
          text: result.text || "",
          objects: result.objects,
          structured_content: result.structured_content,
          metadata: {
            processed_at: new Date().toISOString(),
            ai_provider: "gemini",
            extraction_mode: options.extractionMode,
            page_count: result.metadata?.page_count || 1,
            word_count: result.metadata?.word_count || 0,
            has_text: result.metadata?.has_text ?? Boolean(result.text && result.text.length > 0),
            object_count: result.metadata?.object_count || (result.objects?.length || 0)
          }
        };
        
        return normalizedResult;
      }
      
      return validationResult.data;
    } catch (error) {
      console.error('Result validation failed:', error);
      return result; // Return original result if validation completely fails
    }
  }

  // Enhanced response parsing with better error handling
  private parseAndFormatResponse(response: any, options: ExtractionOptions): ExtractionResult {
    try {
      const responseText = response.response.text() || '{}';
      console.log('Gemini Response Preview:', responseText.substring(0, 300) + '...');
      
      // Clean up response text to ensure it's valid JSON
      const cleanedResponse = responseText
        .replace(/```json\s*/g, '')
        .replace(/```\s*/g, '')
        .replace(/^\s*```\s*/, '')
        .replace(/\s*```\s*$/, '')
        .trim();
      
      let result;
      try {
        result = JSON.parse(cleanedResponse);
      } catch (parseError) {
        // Enhanced fallback parsing
        const jsonMatch = cleanedResponse.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          result = JSON.parse(jsonMatch[0]);
        } else {
          console.error('Failed to parse Gemini response:', cleanedResponse);
          // Return minimal valid structure
          result = {
            text: responseText,
            metadata: {
              processed_at: new Date().toISOString(),
              ai_provider: "gemini",
              extraction_mode: options.extractionMode,
              parsing_error: true
            }
          };
        }
      }
      
      return this.formatResult(result, options);
    } catch (error) {
      console.error('Error parsing Gemini response:', error);
      throw new Error(`Failed to parse Gemini response: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  private getMimeType(filePath: string): string {
    const ext = filePath.toLowerCase().split('.').pop();
    switch (ext) {
      case 'pdf': return 'application/pdf';
      case 'jpg':
      case 'jpeg': return 'image/jpeg';
      case 'png': return 'image/png';
      default: return 'application/octet-stream';
    }
  }

  private buildSystemPrompt(options: ExtractionOptions): string {
    const basePrompt = `You are an expert document data extraction system with advanced OCR, table recognition, and document analysis capabilities. You must generate highly detailed, structured extraction results with positional information and comprehensive analysis.

IMPORTANT: You must carefully analyze the provided document and extract ALL visible information. Do not return empty results unless the document is truly blank.`;
    
    let modePrompt = "";
    switch (options.extractionMode) {
      case 'smart_table':
        modePrompt = `Focus on detecting and extracting tabular data with precise positioning. Preserve table structure, headers, and relationships. Identify table boundaries and maintain cell alignment. Generate detailed analysis of table structure and content.`;
        break;
      case 'full_text':
        modePrompt = `Extract all text content with detailed structural analysis. Include headers, paragraphs, lists, positioning information, and document hierarchy. Provide comprehensive text analysis with positional coordinates.`;
        break;
      case 'form_fields':
        modePrompt = `Identify and extract form fields, labels, and their corresponding values with precise positioning information and detailed field analysis.`;
        break;
      case 'custom':
        modePrompt = `Extract data according to the custom schema provided with detailed positioning and analysis information.`;
        break;
      case 'smart_image':
        modePrompt = `Analyze the image and identify ALL visible objects, items, people, animals, buildings, furniture, vehicles, and other identifiable elements. Also extract any visible text. Focus on providing accurate object labels with confidence scores.`;
        break;
      case 'vlm_layout_aware':
        modePrompt = `Apply Vision-Language Model intelligence for comprehensive document understanding. Analyze textual content, visual features, and spatial relationships. Preserve exact layout with coordinate systems. Extract key-value pairs and semantic structure. Generate intermediate representation with spatial analysis.`;
        break;
    }

    const formatPrompt = options.preserveFormatting 
      ? `üé® ULTRA-PRECISE FORMATTING PRESERVATION:
      - Maintain EXACT character-level spacing, indentation, and alignment
      - Preserve all visual formatting: font styles, sizes, colors, emphasis
      - Capture layout elements: borders, lines, boxes, dividers, backgrounds
      - Maintain spatial relationships: distances, margins, padding, positioning
      - Preserve document structure: sections, columns, blocks, hierarchies
      - Extract with pixel-level coordinate precision for positioning data`
      : `üìä CONTENT-FOCUSED EXTRACTION WITH POSITIONING:
      - Extract core content while preserving essential structural elements
      - Maintain logical hierarchy and document flow
      - Include positional data for spatial context and relationships
      - Preserve key formatting that affects meaning and interpretation`;

    const confidencePrompt = options.includeConfidence
      ? `üìà DETAILED CONFIDENCE ANALYSIS:
      - Provide granular confidence scores (0.0-1.0) for each extracted element
      - Include extraction certainty metrics for text recognition accuracy
      - Assess layout detection confidence and positioning precision
      - Evaluate structural analysis confidence and relationship accuracy
      - Provide overall document processing confidence and quality metrics`
      : `üìà CONFIDENCE ASSESSMENT:
      - Include reasonable confidence estimates for all extracted elements
      - Assess overall extraction quality and accuracy
      - Highlight areas of uncertainty or potential extraction challenges`;

    const structurePrompt = options.extractionMode === 'smart_image' 
      ? `Expected JSON structure:
{
  "objects": [
    {
      "label": "person",
      "confidence": 0.95,
      "description": "A person wearing blue shirt",
      "category": "person"
    }
  ],
  "text": "Any visible text in the image",
  "metadata": {
    "processed_at": "${new Date().toISOString()}",
    "ai_provider": "gemini",
    "extraction_mode": "smart_image",
    "has_text": true,
    "object_count": 2
  }
}`
      : `Expected JSON structure with ENHANCED STRUCTURED FORMAT:
{
  "structured_content": "DETAILED MARKDOWN with comprehensive analysis including:\n- Summary sections with detailed document analysis\n- All text elements with positional coordinates in format: <!-- text, from page X (l=left,t=top,r=right,b=bottom), with ID unique-id -->\n- Tables in proper markdown format with comments: <!-- table, from page X (l=left,t=top,r=right,b=bottom), with ID unique-id -->\n- Figures/images with analysis: <!-- figure, from page X (l=left,t=top,r=right,b=bottom), with ID unique-id -->\n- Marginalia with position info: <!-- marginalia, from page X (l=left,t=top,r=right,b=bottom), with ID unique-id -->\n- Generate unique UUIDs for each element\n- Provide detailed analysis of document structure, content, and layout\n- Include summary of key information found",
  "tables": [
    {
      "title": "Table name or description",
      "confidence": 0.95,
      "data": [
        {"Column1": "Value1", "Column2": "Value2"}
      ],
      "headers": ["Column1", "Column2"]
    }
  ],
  "text": "Any additional text found",
  "metadata": {
    "processed_at": "${new Date().toISOString()}",
    "ai_provider": "gemini",
    "extraction_mode": "${options.extractionMode}",
    "page_count": 1,
    "word_count": 50
  }
}

üö® CRITICAL EXTRACTION REQUIREMENTS:
1. ZERO TOLERANCE FOR MISSING CONTENT - Extract every character, symbol, space, and formatting element
2. PERFECT LAYOUT RECONSTRUCTION - Maintain exact positioning, alignment, and visual hierarchy
3. COMPREHENSIVE POSITIONAL DATA - Include pixel-accurate coordinates for every element
4. FORMAT PRESERVATION - Keep original text formatting, font styles, and emphasis exactly as shown
5. STRUCTURAL INTEGRITY - Preserve document flow, reading order, and element relationships
6. QUALITY VALIDATION - Ensure extraction completeness and accuracy before finalizing
7. METADATA RICHNESS - Provide detailed analysis and confidence metrics for every element

‚ö° MANDATORY OUTPUT QUALITY CHECKS:
- Verify every visible text element has been extracted
- Confirm all tables, lists, and structured data are captured
- Validate formatting preservation and layout accuracy  
- Ensure positional coordinates are precise and complete
- Check that no content has been summarized, paraphrased, or modified
- Confirm original document structure is perfectly reconstructed

üéØ SUCCESS CRITERIA: The extracted markdown should be indistinguishable from the original document in terms of content, structure, and visual presentation when rendered.`;

    return `${basePrompt}\n\n${modePrompt}\n\n${formatPrompt}\n\n${confidencePrompt}\n\n${structurePrompt}\n\nIMPORTANT: Always return valid JSON in the exact structure shown above. If no data is found, still return the structure with empty arrays but include any visible text in the "text" field.`;
  }

  private formatResult(result: any, options: ExtractionOptions): ExtractionResult {
    return {
      tables: result.tables || [],
      text: result.text,
      objects: result.objects || undefined,
      structured_content: result.structured_content,
      metadata: {
        processed_at: new Date().toISOString(),
        ai_provider: "gemini",
        extraction_mode: options.extractionMode,
        page_count: result.metadata?.page_count || 1,
        word_count: result.metadata?.word_count || 0,
        has_text: result.metadata?.has_text,
        object_count: result.metadata?.object_count,
      }
    };
  }
}

export function createAIProvider(provider: 'openai' | 'gemini' | 'enhanced_ocr'): AIProvider {
  switch (provider) {
    case 'openai':
      return new OpenAIProvider();
    case 'gemini':
      return new GeminiProvider();
    case 'enhanced_ocr':
      // Import dynamically to avoid circular dependencies
      const { EnhancedOCRProvider } = require('./enhancedOcrProvider');
      return new EnhancedOCRProvider();
    default:
      throw new Error(`Unsupported AI provider: ${provider}`);
  }
}
